Topic: A Survey of Spiking Neural Networks and Their Applications

Bibliography of Literature Found: (As of March 16, 2025 â€“ To be updated as literature search continues)

ğŸ”µ	Ghosh-Dastidar, S., & Adeli, H. (2009). Spiking neural networks. International journal of neural systems, 19(04), 295-308. https://doi.org/10.1142/S0129065709002002

ğŸ”µ	Tavanaei, A., Ghodrati, M., Kheradpisheh, S. R., Masquelier, T., & Maida, A. (2019). Deep learning in spiking neural networks. Neural networks, 111, 47-63. https://doi.org/10.1016/j.neunet.2018.12.002

ğŸ”µ	Bouvier, M., Valentian, A., Mesquida, T., Rummens, F., Reyboz, M., Vianello, E., & Beigne, E. (2019). Spiking neural networks hardware implementations and challenges: A survey. ACM Journal on Emerging Technologies in Computing Systems (JETC), 15(2), 1-35. https://doi.org/10.1145/3304103

ğŸ”µ	Kudithipudi, D., Schuman, C., Vineyard, C. M., Pandit, T., Merkel, C., Kubendran, R., ... & Furber, S. (2025). Neuromorphic computing at scale. Nature, 637(8047), 801-812. https://doi.org/10.1038/s41586-024-08253-8

ğŸ”µ	Yamazaki, K., Vo-Ho, V.-K., Bulsara, D., & Le, N. (2022). Spiking Neural Networks and Their Applications: A Review. Brain Sciences, 12(7), 863. https://doi.org/10.3390/brainsci12070863

ğŸ”µ	Sanaullah, Koravuna, S., RÃ¼ckert, U., & Jungeblut, T. (2023). Exploring spiking neural networks: A comprehensive analysis of mathematical models and applications. Frontiers in Computational Neuroscience, 17. https://doi.org/10.3389/fncom.2023.1215824

ğŸ”µ	Pietrzak, P., SzczÄ™sny, S., Huderek, D., & Przyborowski, Å. (2023). Overview of Spiking Neural Network Learning Approaches and Their Computational Complexities. Sensors, 23(6), 3037. https://doi.org/10.3390/s23063037

ğŸ”µ	Maass, W. (1997). Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9), 1659-1671. https://doi.org/10.1016/S0893-6080(97)00011-7

ğŸ”µ	Benjamin, B. V., Gao, P., McQuinn, E., Choudhary, S., Chandrasekaran, A. R., Bussat, J. M., ... & Boahen, K. (2014). Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations. Proceedings of the IEEE, 102(5), 699-716. https://doi.org/10.1109/JPROC.2014.2313565

ğŸ”µ	Davies, M., Srinivasa, N., Lin, T. H., Chinya, G., Cao, Y., Choday, S. H., ... & Wang, H. (2018). Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1), 82-99. https://doi.org/10.1109/MM.2018.112130359

ğŸ”µ	Furber, S. B., Galluppi, F., Temple, S., & Plana, L. A. (2014). The spinnaker project. Proceedings of the IEEE, 102(5), 652-665. https://doi.org/10.1109/JPROC.2014.2304638

ğŸ”µ	Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., ... & Modha, D. S. (2014). A million spiking-neuron integrated circuit with a scalable communication network and interface. Science, 345(6197), 668-673. https://doi.org/10.1126/science.1254642

ğŸ”µ	Schemmel, J., BrÃ¼derle, D., GrÃ¼bl, A., Hock, M., Meier, K., & Millner, S. (2010, May). A wafer-scale neuromorphic hardware system for large-scale neural modeling. In 2010 IEEE international symposium on circuits and systems (iscas) (pp. 1947-1950). IEEE. https://doi.org/10.1109/ISCAS.2010.5536970

ğŸ”µ	Diehl, P. U., & Cook, M. (2015). Unsupervised learning of digit recognition using spike-timing-dependent plasticity. Frontiers in computational neuroscience, 9, 99. https://doi.org/10.3389/fncom.2015.00099

ğŸ”µ	Mostafa, H., Pedroni, B. U., Sheik, S., & Cauwenberghs, G. (2017, May). Fast classification using sparsely active spiking networks. In 2017 IEEE International Symposium on Circuits and Systems (ISCAS) (pp. 1-4). IEEE. https://doi.org/10.1109/ISCAS.2017.8050527

ğŸ”µ	Srinivasan, G., & Roy, K. (2019). Restocnet: Residual stochastic binary convolutional spiking neural network for memory-efficient neuromorphic computing. Frontiers in neuroscience, 13, 189. https://doi.org/10.3389/fnins.2019.00189

ğŸ”µ	Zhang, M., Gu, Z., Zheng, N., Ma, D., & Pan, G. (2020). Efficient spiking neural networks with logarithmic temporal coding. IEEE access, 8, 98156-98167. https://doi.org/10.1109/ACCESS.2020.2994360

ğŸ”µ	Zhang, W., & Li, P. (2020). Temporal spike sequence learning via backpropagation for deep spiking neural networks. Advances in neural information processing systems, 33, 12022-12033. https://doi.org/10.48550/arXiv.2002.10085

ğŸ”µ	Rueckauer, B., & Liu, S. C. (2018, May). Conversion of analog to spiking neural networks using sparse temporal coding. In 2018 IEEE international symposium on circuits and systems (ISCAS) (pp. 1-5). IEEE. https://doi.org/10.1109/ISCAS.2018.8351295

ğŸ”µ	Han, B., & Roy, K. (2020, August). Deep spiking neural network: Energy efficiency through time based coding. In European conference on computer vision (pp. 388-404). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-58607-2_23

ğŸ”µ	Sengupta, A., Ye, Y., Wang, R., Liu, C., & Roy, K. (2019). Going deeper in spiking neural networks: VGG and residual architectures. Frontiers in neuroscience, 13, 95. https://doi.org/10.3389/fnins.2019.00095

ğŸ”µ	Rathi, N., Srinivasan, G., Panda, P., & Roy, K. (2020). Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation. arXiv preprint arXiv:2005.01807. https://doi.org/10.48550/arXiv.2005.01807

ğŸ”µ	Rueckauer, B., Lungu, I. A., Hu, Y., Pfeiffer, M., & Liu, S. C. (2017). Conversion of continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in neuroscience, 11, 682. https://doi.org/10.3389/fnins.2017.00682

ğŸ”µ	Lee, C., Sarwar, S. S., Panda, P., Srinivasan, G., & Roy, K. (2020). Enabling spike-based backpropagation for training deep neural network architectures. Frontiers in neuroscience, 14, 497482. https://doi.org/10.3389/fnins.2020.00119

ğŸ”µ	Wu, Y., Deng, L., Li, G., Zhu, J., Xie, Y., & Shi, L. (2019, July). Direct training for spiking neural networks: Faster, larger, better. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 1311-1318). https://doi.org/10.1609/aaai.v33i01.33011311

ğŸ”µ	Rathi, N., & Roy, K. (2020). Diet-snn: Direct input encoding with leakage and threshold optimization in deep spiking neural networks. arXiv preprint arXiv:2008.03658. https://doi.org/10.48550/arXiv.2008.03658

ğŸ”µ	Han, B., Srinivasan, G., & Roy, K. (2020). Rmp-snn: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 13558-13567). https://doi.org/10.48550/arXiv.2003.01811

ğŸ”µ	Lu, S., & Sengupta, A. (2020). Exploring the connection between binary and spiking neural networks. Frontiers in neuroscience, 14, 535. https://doi.org/10.3389/fnins.2020.00535

ğŸ”µ	Lee, J. H., Delbruck, T., & Pfeiffer, M. (2016). Training deep spiking neural networks using backpropagation. Frontiers in neuroscience, 10, 508. https://doi.org/10.3389/fnins.2016.00508

ğŸ”µ	Wu, Y., Deng, L., Li, G., Zhu, J., & Shi, L. (2018). Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12, 331. https://doi.org/10.3389/fnins.2018.00331

ğŸ”µ	Shrestha, S. B., & Orchard, G. (2018). Slayer: Spike layer error reassignment in time. Advances in neural information processing systems, 31. https://doi.org/10.48550/arXiv.1810.08646

ğŸ”µ	Jin, Y., Zhang, W., & Li, P. (2018). Hybrid macro/micro level backpropagation for training deep spiking neural networks. Advances in neural information processing systems, 31. https://doi.org/10.48550/arXiv.1805.07866

ğŸ”µ	Kugele, A., Pfeil, T., Pfeiffer, M., & Chicca, E. (2020). Efficient processing of spatio-temporal data streams with spiking neural networks. Frontiers in neuroscience, 14, 512192. https://doi.org/10.3389/fnins.2020.00439

ğŸ”µ	Wu, H., Zhang, Y., Weng, W., Zhang, Y., Xiong, Z., Zha, Z. J., ... & Wu, F. (2021, May). Training spiking neural networks with accumulated spiking flow. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 12, pp. 10320-10328). https://doi.org/10.1609/aaai.v35i12.17236

ğŸ”µ	Aayush Ankit, Abhronil Sengupta, Priyadarshini Panda, and Kaushik Roy. 2017. RESPARC: A reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks. In 54th Annual Design Automation Conference. ACM, 27. https://doi.org/10.48550/arXiv.1702.06064

ğŸ”µ	Alireza Bagheri, Osvaldo Simeone, and Bipin Rajendran. 2018. Adversarial training for probabilistic spiking neural networks. In IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). IEEE, 1â€“5. https://doi.org/10.48550/arXiv.1802.08567

ğŸ”µ	Gupta, A., & Long, L. N. (2007, August). Character recognition using spiking neural networks. In 2007 International Joint Conference on Neural Networks (pp. 53-58). IEEE. https://doi.org/10.1109/IJCNN.2007.4370930

ğŸ”µ	Liang, L., Hu, X., Deng, L., Wu, Y., Li, G., Ding, Y., ... & Xie, Y. (2021). Exploring adversarial attack in spiking neural networks with spike-compatible gradient. IEEE transactions on neural networks and learning systems, 34(5), 2569-2583. https://doi.org/10.48550/arXiv.2001.01587

ğŸ”µ	Neftci, E. O., Mostafa, H., & Zenke, F. (2019). Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6), 51-63. https://doi.org/10.1109/MSP.2019.2931595

ğŸ”µ	Wang, X., Lin, X., & Dang, X. (2020). Supervised learning in spiking neural networks: A review of algorithms and evaluations. Neural Networks, 125, 258-280. https://doi.org/10.1016/j.neunet.2020.02.011
