Bibliography of Literature Found:
(As of March 5, 2025 – To be updated as literature search continues)

1. Mahmud, H., Lama, P., Desai, K., & Prasad, S. K. (2024, December). EncodeNet: A Framework for Boosting DNN Accuracy with Entropy-Driven Generalized Converting Autoencoder. In International Conference on Pattern Recognition (pp. 463-477). Cham: Springer Nature Switzerland. https://doi.org/10.48550/arXiv.2404.13770

2. Mahmud, H., Kang, P., Lama, P., Desai, K., & Prasad, S. K. (2024, June). CAE-Net: Enhanced Converting Autoencoder Based Framework for Low-Latency Energy-Efficient DNN with SLO-Constraints. In 2024 IEEE Cloud Summit (pp. 128-134). IEEE. http://dx.doi.org/10.1109/Cloud-Summit61220.2024.00028

3. Mahmud, H., Kang, P., Desai, K., Lama, P., & Prasad, S. K. (2024, May). A converting autoencoder toward low-latency and energy-efficient DNN inference at the edge. In 2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (pp. 592-599). IEEE. https://doi.org/10.48550/arXiv.2403.07036

4. Rathi, N., Chakraborty, I., Kosta, A., Sengupta, A., Ankit, A., Panda, P., & Roy, K. (2023). Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware. ACM Computing Surveys, 55(12), 1-49. https://doi.org/10.1145/3571155

5. Kudithipudi, D., Schuman, C., Vineyard, C. M., Pandit, T., Merkel, C., Kubendran, R., ... & Furber, S. (2025). Neuromorphic computing at scale. Nature, 637(8047), 801-812. https://doi.org/10.1038/s41586-024-08253-8

6. Yamazaki, K., Vo-Ho, V.-K., Bulsara, D., & Le, N. (2022). Spiking Neural Networks and Their Applications: A Review. Brain Sciences, 12(7), 863. https://doi.org/10.3390/brainsci12070863

7. Sanaullah, Koravuna, S., Rückert, U., & Jungeblut, T. (2023). Exploring spiking neural networks: A comprehensive analysis of mathematical models and applications. Frontiers in Computational Neuroscience, 17. https://doi.org/10.3389/fncom.2023.1215824

8. Pietrzak, P., Szczęsny, S., Huderek, D., & Przyborowski, Ł. (2023). Overview of Spiking Neural Network Learning Approaches and Their Computational Complexities. Sensors, 23(6), 3037. https://doi.org/10.3390/s23063037

9. Aayush Ankit, Abhronil Sengupta, Priyadarshini Panda, and Kaushik Roy. 2017. RESPARC: A reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks. In 54th Annual Design Automation Conference. ACM, 27. https://doi.org/10.48550/arXiv.1702.06064

10. Alireza Bagheri, Osvaldo Simeone, and Bipin Rajendran. 2018. Adversarial training for probabilistic spiking neural networks. In IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC). IEEE, 1–5. https://doi.org/10.48550/arXiv.1802.08567

11. Ankur Gupta and Lyle N. Long. 2007. Character recognition using spiking neural networks. In International Joint Conference on Neural Networks. IEEE, 53–58. https://doi.org/10.1109/IJCNN.2007.4370930

12. Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. 2020. RMP-SNNs: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.48550/arXiv.2003.01811

13. Yingyezhe Jin,Wenrui Zhang, and Peng Li. 2018. Hybrid macro/micro level backpropagation for training deep spiking neural networks. In Advances in Neural Information Processing Systems 31. Curran Associates, Inc., 7005–7015. https://doi.org/10.48550/arXiv.1805.07866

14. Ling Liang, Xing Hu, Lei Deng, Yujie Wu, Guoqi Li, Yufei Ding, Peng Li, and Yuan Xie. 2020. Exploring adversarial attack in spiking neural networks with spike-compatible gradient. arXiv preprint arXiv:2001.01587 (2020). https://doi.org/10.48550/arXiv.2001.01587

15. Emre O. Neftci, Hesham Mostafa, and Friedemann Zenke. 2019. Surrogate gradient learning in spiking neural networks:
Bringing the power of gradient-based optimization to spiking neural networks. IEEE Sig. Process. Mag. 36, 6 (2019), 51–63. https://doi.org/10.1109/MSP.2019.2931595
